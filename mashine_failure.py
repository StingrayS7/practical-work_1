# -*- coding: utf-8 -*-
"""3_task_creative(mashine_failure)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1y470QEI5z0BoaxWAX_8HcHCWZbWp5-vp

Задача: создать и обучить модель для предсказания событий выхода из строя оборудования на производстве, опираясь на показатели датчиков.
"""

import pandas as pd
from requests import get
import tensorflow as tf
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

response = get("https://drive.usercontent.google.com/u/0/uc?id=1BmkDUndkHmXgJ_nRIsYp5Kvw0BxeFHzk&export=download")
with open('archive.zip', 'wb') as f:
    f.write(response.content)

!unzip -qo "archive.zip" -d ./machine_failure

FILE_PATH = './machine_failure'

data = pd.read_csv(f'{FILE_PATH}/data.csv')

# Иформация о столбцах датасета
data.info()

"""Описание показателей

*   footfall: Количество людей или объектов, проходящих мимо оборудования
*   tempMode: Температурный режим оборудования
*   AQ: Индекс качества воздуха рядом с оборудованием
*   USS: Данные ультразвукового датчика приближения
*   CS: данные датчика потребления тока Оборудованием
*   VOC: Уровень летучих органических соединений рядом с оборудованием
*   RP: Частота вращения исполнительных элементов оборудования
*   IP: Входящее давление на оборудование
*   Temperature: Рабочая температура оборудования
*   fail: Бинарный (двоичный) индикатор исправности или неисправности оборудования (1 - неисправность, 0 - нет неисправности)







"""

# Проверка начальных значений датасета
data.head()

# Обработка данных. Поиск нулевых значений в датасете.
miss_val = data.isnull().sum()
print(miss_val.to_markdown())

# Нулевых значений в датасете нет
# Создаем матрицу корреляций для выбора признаков, по которым лучше всего делать предсказывание
corr = data.corr()
plt.figure(figsize=(10,10))
sns.heatmap(corr, cmap="viridis", annot=True,)

# Лучшими показателями судя по матрице корреляций будут AQ и VOC
fin_data = data[['AQ', 'Temperature', 'fail']]
fin_data.head(20)

# Подготовим тренировочные и тестовые данные
X = fin_data.drop('fail', axis=1).values
y = fin_data['fail'].values
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=20)

# Создание модели нейронной сети
model = tf.keras.models.Sequential(
    [
        tf.keras.layers.Dense(4, activation='relu'),
        tf.keras.layers.Dense(8, activation='relu'),
        tf.keras.layers.Dense(8, activation='relu'),
        tf.keras.layers.Dense(1, activation='sigmoid')
    ]
)

# Модель 4 слоя, оптимизатор Adam, активаторы relu и sigmoid, функция потерь - бинарная перекрестная энтропия
model.compile(optimizer='Adam', loss='binary_crossentropy', metrics=['accuracy'])

# Тренируем модель 100 эпох
model.fit(X_train, y_train, validation_split=0.2, epochs=100)

# Проверяем модель на тестовых данных
res = model.evaluate(X_test, y_test)
print('Тестовая ошибка, тестовая точность:', res)

"""При обучении модели на данном датасете можно наблюдать высокий показатель обучающей и валидационной ошибок, а так же недостаточную точность модели. Возможное решение: использование более крупного датасета."""